---
AWSTemplateFormatVersion: 2010-09-09
Description: This is a CloudFormation template to create infrastructure to send logs that are put into an S3 bucket to Splunk.

Parameters:

  service:
    Type: String
    Description: Service name used in tagging AWS resources.
    Default: splunk-aws-gdi-toolkit

  stage:
    Type: String
    Description: Used to distinguish between stages of an environment (dev, test, prod, stage, etc).  Only used in AWS resource tagging.
    Default: dev

  contact:
    Description: Used to identify a contact for the resources created in this stack.  Only used in AWS resource tagging.  As an example, this could be an email address or username.
    Type: String
    Default: ""

  logType:
    Type: String
    Description: Log type, used in AWS resource names.  Must follow the normal rules for S3 bucket names (https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html), but also less than 30 characters long.
    AllowedPattern: "(?!(xn--|-s3alias$))^[a-z0-9][a-z0-9-]{1,20}[a-z0-9]$"

  existingS3BucketName:
    Type: String
    Description: Name of the existng S3 bucket you want to ingest data from.  Leave this blank to create a new S3 bucket.  If you specify the name of an existing S3 bucket, you'll need to create the S3 > SQS notification settings.
    Default: ""

  s3ObjectExpirationInDays:
    Description: How many days to keep the files in S3 before they're deleted.  This only applies to new buckets and not existing buckets.
    Default: 90
    Type: String

  roleAccessToS3BucketPut:
    Type: String
    Description: ARN of the role to allow the putting of objects in the S3 bucket.  Leave blank to auto-detect appropriate settings for CloudTrail, ELB Access, or VPC Flow logs.
    Default: ""

  sqsQueueVisibilityTimeoutInSecond:
    Type: Number
    Description: How long to let SQS messages be taken by the Lambda function before they become available to be processed again.  Must be more than lambdaProcessorTimeout.
    Default: 630
    MinValue: 0
    MaxValue: 43200

  splunkHECEndpoint:
    Type: String
    Description: Destination URL that Firehose will send data to.

  splunkHECToken:
    Type: String
    Description: HEC token Firehose will use to authenticate data being sent to Splunk.

  lambdaProcessorMemorySize:
    Type: Number
    Description: Size of memory to allocate to Lambda function.
    Default: 1024
    MinValue: 128
    MaxValue: 10240

  lambdaProcessorTimeout:
    Type: Number
    Description: How long the Lambda function can run until it times out.
    Default: 300
    MinValue: 1
    MaxValue: 900

  lambdaProcessorBatchSize:
    Type: Number
    Description: How many SQS messages (aka files from S3) to process in a single Lambda execution.
    Default: 100
    MinValue: 1
    MaxValue: 10000

  lambdaProcessorBatchingWindowInSeconds:
    Type: Number
    Description: How long to let SQS messages (aka files from S3) queue up before processing them.
    Default: 15
    MinValue: 0
    MaxValue: 300

  splunkIndex:
    Type: String
    Description: Name of the index in Splunk events will be sent to.

  splunkSourcetype:
    Type: String
    Description: Name of the sourcetype for the events that will be sent to Splunk.

  splunkSource:
    Type: String
    Description: Name of the source for the events that will be sent to Splunk.

  splunkHost:
    Type: String
    Description: Name of the host for the events that will be sent to Splunk.

  splunkTimePrefix:
    Type: String
    Description: Text right before the time stamp in the event that will be used to help identify the timestamp.  Leave empty if the event is delineated.
    Default: ""

  splunkEventDelimiter:
    Type: String
    Description: Text that is used to split the fields in each event.  Leave empty if the log file is not delineated.
    Default: ""

  splunkTimeDelineatedField:
    Type: String
    Description: Number associated with a delineated event that corresponds with the timestamp.  Leave empty if the log file is not delineated.
    Default: ""

  splunkTimeFormat:
    Type: String
    Description: How the timestamp in the event is formatted.
    Default: prefix-ISO8601
    AllowedValues:
      - prefix-ISO8601
      - prefix-epoch
      - delineated-epoch
      - delineated-ISO8601
      - delineated-strftime

  splunkStrfTimeFormat:
    Type: String
    Description: Strftime format of timestamp.  Leave empty if the timestamp does not need to be specified in strftime.
    Default: ""

  splunkJSONFormat:
    Type: String
    Description: How the JSON events are formatted.  Leave default if the events are not in JSON format.
    Default: NDJSON
    AllowedValues:
      - eventsInRecords
      - NDJSON

  splunkIgnoreFirstLine:
    Type: String
    Description: Whether the first line of the log file should be discarded.
    Default: false
    AllowedValues:
      - true
      - false

  splunkCSVToJSON:
    Type: String
    Description: Whether the to use the first line in the CSV file to convert the file from CSV to JSON.  If the file is not CSV-formatted, this can be ignored and left at the default setting.
    Default: false
    AllowedValues:
      - true
      - false

  splunkRemoveEmptyCSVToJsonFields:
    Type: String
    Description: Whether the to remove empty or null JSON fields after converting the event from CSV to JSON.  If the file is not CSV-formatted, this can be ignored and left at the default setting.
    Default: true
    AllowedValues:
      - true
      - false

  cloudWatchAlertEmail:
    Type: String
    Description: Email address for receiving alerts related to CloudTrail ingestion.  Leave empty for no alerting.
    Default: ""


Conditions:
  createNewS3Bucket: !Equals
    - !Ref existingS3BucketName
    - ""
  useExistingS3Bucket: !Not
    - !Equals
      - !Ref existingS3BucketName
      - ""
  cloudTrail: !Equals
    - !Ref logType
    - "cloudtrail"
  cloudTrailNewBucket: !And
    - !Condition cloudTrail
    - !Condition createNewS3Bucket
  vpcFlowLog: !Equals
    - !Ref logType
    - "vpcflowlog"
  vpcFlowLogNewBucket: !And
    - !Condition vpcFlowLog
    - !Condition createNewS3Bucket
  elbLog: !Equals
    - !Ref logType
    - "elblog"
  elbLogNewBucket: !And
    - !Condition elbLog
    - !Condition createNewS3Bucket
  route53Log: !Equals
    - !Ref logType
    - "route53"
  route53LogNewBucket: !And
    - !Condition route53Log
    - !Condition createNewS3Bucket
  billingLog: !Equals
    - !Ref logType
    - "billingcur"
  billingNewBucket: !And
    - !Condition billingLog
    - !Condition createNewS3Bucket
  s3ServerAccessLog: !Equals
    - !Ref logType
    - "s3serveraccesslog"
  s3ServerAccessLogNewBucket: !And
    - !Condition s3ServerAccessLog
    - !Condition createNewS3Bucket
  customLogType: !And
    - !Not
       - !Condition cloudTrail
    - !Not
       - !Condition vpcFlowLog
    - !Not
       - !Condition elbLog
    - !Not
       - !Condition route53Log
    - !Not
       - !Condition billingLog
    - !Not
       - !Condition s3ServerAccessLog
  customLogTypeNewBucket: !And
    - !Condition customLogType
    - !Condition createNewS3Bucket
  enableAlerting: !Not 
    - !Equals 
      - !Ref cloudWatchAlertEmail
      - ""


Mappings:
  RegionMapping:
    us-east-1:
      elbAccountIDPrincipal: arn:aws:iam::127311923021:root
    us-east-2:
      elbAccountIDPrincipal: arn:aws:iam::033677994240:root
    us-west-1:
      elbAccountIDPrincipal: arn:aws:iam::027434742980:root
    us-west-2:
      elbAccountIDPrincipal: arn:aws:iam::797873946194:root
    af-south-1:
      elbAccountIDPrincipal: arn:aws:iam::098369216593:root
    ca-central-1:
      elbAccountIDPrincipal: arn:aws:iam::985666609251:root
    eu-central-1:
      elbAccountIDPrincipal: arn:aws:iam::054676820928:root
    eu-west-1:
      elbAccountIDPrincipal: arn:aws:iam::156460612806:root
    eu-west-2:
      elbAccountIDPrincipal: arn:aws:iam::652711504416:root
    eu-south-1:
      elbAccountIDPrincipal: arn:aws:iam::635631232127:root
    eu-west-3:
      elbAccountIDPrincipal: arn:aws:iam::009996457667:root
    eu-north-1:
      elbAccountIDPrincipal: arn:aws:iam::897822967062:root
    ap-east-1:
      elbAccountIDPrincipal: arn:aws:iam::754344448648:root
    ap-northeast-1:
      elbAccountIDPrincipal: arn:aws:iam::582318560864:root
    ap-northeast-2:
      elbAccountIDPrincipal: arn:aws:iam::600734575887:root
    ap-northeast-3:
      elbAccountIDPrincipal: arn:aws:iam::383597477331:root
    ap-southeast-1:
      elbAccountIDPrincipal: arn:aws:iam::114774131450:root
    ap-southeast-2:
      elbAccountIDPrincipal: arn:aws:iam::783225319266:root
    ap-southeast-3:
      elbAccountIDPrincipal: arn:aws:iam::589379963580:root
    ap-south-1:
      elbAccountIDPrincipal: arn:aws:iam::718504428378:root
    me-south-1:
      elbAccountIDPrincipal: arn:aws:iam::076674570225:root
    sa-east-1:
      elbAccountIDPrincipal: arn:aws:iam::507241528517:root
    us-gov-west-1:
      elbAccountIDPrincipal: arn:aws:iam::048591011584:root
    us-gov-east-1:
      elbAccountIDPrincipal: arn:aws:iam::190560391635:root
    cn-north-1:
      elbAccountIDPrincipal: arn:aws:iam::638102146993:root
    cn-northwest-1:
      elbAccountIDPrincipal: arn:aws:iam::037604701340:root


Resources:
  # S3 resources
  s3Bucket:
    Type: AWS::S3::Bucket
    Condition: createNewS3Bucket
    Properties:
      AccessControl: Private
      BucketEncryption: 
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      BucketName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}"
      LifecycleConfiguration:
        Rules:
            - Id: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-cleanup"
              AbortIncompleteMultipartUpload:
                DaysAfterInitiation: 1
              Status: Enabled
            - Id: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-expiration"
              ExpirationInDays: !Ref s3ObjectExpirationInDays
              Status: Enabled
      NotificationConfiguration:
        QueueConfigurations:
          - Event: "s3:ObjectCreated:*"
            Queue: !GetAtt s3BucketNotificationSQSQueue.Arn
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact
      - Key: logType
        Value: !Ref logType

  s3BucketPolicyCloudTrailLogs:
    Type: AWS::S3::BucketPolicy
    Condition: cloudTrailNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            Service: cloudtrail.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: cloudtrail.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"

  s3BucketPolicyVPCFlowLogs:
    Type: AWS::S3::BucketPolicy
    Condition: vpcFlowLogNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            Service: delivery.logs.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: delivery.logs.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              s3:x-amz-acl: bucket-owner-full-control

  s3BucketPolicyELBLogs:
    Type: AWS::S3::BucketPolicy
    Condition: elbLogNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            Service: logdelivery.elb.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            AWS: !FindInMap [RegionMapping, !Ref AWS::Region, elbAccountIDPrincipal]
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              s3:x-amz-acl: bucket-owner-full-control

  s3BucketPolicyRoute53Logs:
    Type: AWS::S3::BucketPolicy
    Condition: route53LogNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            Service: delivery.logs.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: delivery.logs.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              s3:x-amz-acl: bucket-owner-full-control
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            AWS: !Sub "arn:aws:iam::${AWS::AccountId}:root"
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"

  s3BucketPolicyBillingLogs:
    Type: AWS::S3::BucketPolicy
    Condition: billingNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
            - s3:GetBucketPolicy
          Principal: 
            Service: billingreports.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
          Condition:
            StringEquals:
              aws:SourceArn: !Sub "arn:aws:cur:us-east-1:${AWS::AccountId}:definition/*"
              aws:SourceAccount: !Ref AWS::AccountId
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: billingreports.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              aws:SourceArn: !Sub "arn:aws:cur:us-east-1:${AWS::AccountId}:definition/*"
              aws:SourceAccount: !Ref AWS::AccountId

  s3BucketPolicyS3ServerAccessLogs:
    Type: AWS::S3::BucketPolicy
    Condition: s3ServerAccessLogNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Sid: "S3ServerAccessLogsPolicy"
          Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: logging.s3.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              aws:SourceAccount: !Ref AWS::AccountId

  s3BucketPolicyCustomRole:
    Type: AWS::S3::BucketPolicy
    Condition: customLogTypeNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            AWS: !Ref roleAccessToS3BucketPut
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            AWS: !Ref roleAccessToS3BucketPut
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"

  # S3 Notifications > SQS
  s3BucketNotificationSQSQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-sqs-queue"
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact
      - Key: logType
        Value: !Ref logType
      VisibilityTimeout: !Ref sqsQueueVisibilityTimeoutInSecond

  s3BucketNotificationSQSQueuePolicy: 
    Type: AWS::SQS::QueuePolicy
    Properties: 
      PolicyDocument:
        Version: 2012-10-17
        Id: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-sqs-queuePolicy"
        Statement:
        -
          Sid: Send messages to SQS
          Effect: Allow
          Principal:
            AWS: "*"
          Action:
            - "SQS:SendMessage"
          Resource: "*"
          Condition:
            ArnLike: 
              "aws:SourceARN": !If [useExistingS3Bucket, !Sub "arn:aws:s3:::${existingS3BucketName}", !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"]
      Queues:
        - !Ref "s3BucketNotificationSQSQueue"

  # Firehose > Splunk resources
  firehose:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties: 
      DeliveryStreamName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
      DeliveryStreamType: DirectPut
      SplunkDestinationConfiguration:
        BufferingHints:
          IntervalInSeconds: 5
          SizeInMBs: 1
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref firehoseLogGroup
          LogStreamName: "SplunkDelivery"
        HECAcknowledgmentTimeoutInSeconds: 300
        HECEndpoint: !Ref splunkHECEndpoint
        HECEndpointType: "Event"
        HECToken: !Ref splunkHECToken
        RetryOptions:
          DurationInSeconds: 3600
        S3BackupMode: "FailedEventsOnly"
        S3Configuration:
          BucketARN: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}-firehose-backsplash"
          BufferingHints:
            IntervalInSeconds: 300
            SizeInMBs: 16
          CompressionFormat: "GZIP"
          Prefix: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
          RoleARN: !GetAtt firehoseIAMRole.Arn

  firehoseIAMPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action:
          - logs:Describe*
          - logs:PutLogEvents
          Resource: !GetAtt firehoseLogGroup.Arn
        - Effect: Allow
          Action:
          - s3:PutObject
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}-firehose-backsplash/*"
      ManagedPolicyName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-iam-policy"

  firehoseIAMRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: "Allow"
          Principal:
            Service: "firehose.amazonaws.com"
          Action:
            - sts:AssumeRole
      ManagedPolicyArns:
        - !Ref firehoseIAMPolicy
      RoleName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-iam-role"
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact

  firehoseBacksplashBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-backsplash"
      AccessControl: Private
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
            - Id: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-backsplash-cleanup"
              AbortIncompleteMultipartUpload:
                DaysAfterInitiation: 1
              Status: Enabled
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact

  firehoseLogGroup: 
    Type: AWS::Logs::LogGroup
    Properties: 
      LogGroupName: !Sub "/aws/kinesisfirehose/${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
      RetentionInDays: 7

  firehoseLogStream:
    Type: AWS::Logs::LogStream
    Properties: 
      LogGroupName: !Ref firehoseLogGroup
      LogStreamName: "SplunkDelivery"


# Lambda resources
  lambdaFunction:
    Type: AWS::Lambda::Function
    DependsOn: lambdaLogGroup
    Properties:
      Architectures:
        - arm64
      Code:
        ZipFile: |
          import boto3, gzip, json, os, sys, shutil, re, dateutil.parser, time, csv, datetime, pandas, pyarrow, urllib.parse, random

          # AWS-related setup
          s3Client = boto3.client('s3')
          firehoseDeliverySreamName = os.environ['firehoseDeliverySreamName']
          firehoseClient = boto3.client('firehose', region_name=os.environ['AWS_REGION'])
          recordBatch = []

          # Splunk-related setup
          SPLUNK_INDEX = os.environ['SPLUNK_INDEX']
          SPLUNK_TIME_PREFIX = os.environ['SPLUNK_TIME_PREFIX']
          SPLUNK_EVENT_DELIMITER = os.environ['SPLUNK_EVENT_DELIMITER']
          SPLUNK_TIME_DELINEATED_FIELD = os.environ['SPLUNK_TIME_DELINEATED_FIELD']
          SPLUNK_TIME_FORMAT = os.environ['SPLUNK_TIME_FORMAT']
          SPLUNK_STRFTIME_FORMAT = os.environ['SPLUNK_STRFTIME_FORMAT']
          SPLUNK_SOURCETYPE = os.environ['SPLUNK_SOURCETYPE']
          SPLUNK_SOURCE = os.environ['SPLUNK_SOURCE']
          SPLUNK_HOST = os.environ['SPLUNK_HOST']
          SPLUNK_JSON_FORMAT = os.environ['SPLUNK_JSON_FORMAT']
          SPLUNK_CSV_TO_JSON = os.environ['SPLUNK_CSV_TO_JSON']
          SPLUNK_IGNORE_FIRST_LINE = os.environ['SPLUNK_IGNORE_FIRST_LINE']
          SPLUNK_REMOVE_EMPTY_CSV_TO_JSON_FIELDS = os.environ['SPLUNK_REMOVE_EMPTY_CSV_TO_JSON_FIELDS']

          # Lambda things
          validFileTypes = ["gz", "gzip", "json", "csv", "log", "parquet", "txt", "ndjson", "jsonl"]
          unsupportedFileTypes = ["CloudTrail-Digest", "billing-report-Manifest"]
          delimiterMapping = {"space": " ", "tab": "  ", "comma": ",", "semicolon": ";"}
          maxRetriesToFirehose = 11

          # Create delimiter for delimiting events
          def createDelimiter(SPLUNK_EVENT_DELIMITER):

            if SPLUNK_EVENT_DELIMITER in delimiterMapping.keys():
              return delimiterMapping[SPLUNK_EVENT_DELIMITER]
            else:
              return SPLUNK_EVENT_DELIMITER


          # Parse SQS message for bucket information
          def retrieveObjectInfo(record):
            
            # Try to parse the record for file information
            try:
              record = json.loads(record['body'])
              bucket = record['Records'][0]['s3']['bucket']['name']
              key = urllib.parse.unquote_plus(record['Records'][0]['s3']['object']['key'])

              # Construct and return the result
              result = {}
              result["bucket"] = bucket
              result["key"] = key
              return result

            # Return an error if the record doesn't have a valid file defined in it
            except:
              return("SQS message did not contain S3 file information.  Record: " + str(record))


          # Check to see if the file is a valid file type
          def isValidFileType(key):

            # Check for invalid file types
            for unsupportedFileType in unsupportedFileTypes:
              if (unsupportedFileType in key):
                return False

            # Define file extension
            extension = key.split(".")[-1]

            # Check for valid file types
            if extension in validFileTypes:
              return True

            # Check for aws:s3:accesslogs
            if SPLUNK_SOURCETYPE == "aws:s3:accesslogs" and len(key.split(".")) == 1:
              return True

            return False


          # Retrieve the S3 object, and return the new path
          def downloadS3Object(bucket, key):

            try:
              # Define the path for the file
              path = "/tmp/" + key.split("/")[-1]

              # Download the file from the S3 bucket
              s3Client.download_file(bucket, key, path)

              # Return the new file path
              return(path)

            except:
              return "Unable to download file s3://" + bucket + "/" + key


          # Uncompress the file if it needs to be uncompressed, then return the path and the new file extension
          def uncompressFile(path):

            # Set file extension and new file path (if it gets uncompressed)
            extension = path.split(".")[-1]
            uncompressedFilePath = path[0:(-1*(len(extension)) - 1)]

            try:
              match extension:
                case "gz":

                  with gzip.open(path, 'rb') as f_in:
                    with open(uncompressedFilePath, 'wb') as f_out:
                      shutil.copyfileobj(f_in, f_out)

                  # Remove the uncompressed file
                  os.remove(path)

                  return uncompressedFilePath
                case "gzip":

                  with gzip.open(path, 'rb') as f_in:
                    with open(uncompressedFilePath, 'wb') as f_out:
                      shutil.copyfileobj(f_in, f_out)

                  # Remove the uncompressed file
                  os.remove(path)

                  return uncompressedFilePath
                case "parquet":

                  df = pandas.read_parquet(path)
                  json_array = df.to_json(orient='records', lines=True)
                  uncompressedFilePath = uncompressedFilePath + ".json"
                  with open(uncompressedFilePath, "w") as f_out:
                    f_out.write(json_array)

                  # Remove the uncompressed file
                  os.remove(path)

                  return uncompressedFilePath

            except:
              return "Unable to uncompress file"

            return path


          # Split events into a list. Additional file extensions should be added here.
          def eventBreak(events, extension, ignoreFirstLine):

            if extension == "csv" or extension == "log" or SPLUNK_SOURCETYPE == "aws:s3:accesslogs":

              splitEvents = events.split("\n")

              # Remove empty last line if it exists
              if len(splitEvents[-1]) == 0:
                splitEvents = splitEvents[:-1]

              if ignoreFirstLine == "true":
                splitEvents = splitEvents[1:]
              
              events = ""

              return splitEvents

            elif extension == "json" or extension == "txt" or extension=="jsonl":

              if SPLUNK_JSON_FORMAT == "eventsInRecords":
                splitEvents = json.loads(events)["Records"]
                events = ""

                return splitEvents

              elif SPLUNK_JSON_FORMAT == "NDJSON":
                splitEvents = events.split("\n")
                events = ""
                
                if len(splitEvents[-1]) == 0:
                  splitEvents = splitEvents[:-1]

                return splitEvents

            else: 
              return "File type invalid"


          # Clean up first line 
          def cleanFirstLine(splitEvents):

            # If the sourcetype is aws:billing:cur, remove everything before the "/" in the CSV header
            if SPLUNK_SOURCETYPE == "aws:billing:cur":
              
              header = splitEvents[0]
              
              newHeader = ""
              
              for splitHeader in header.split(","):
                newHeader += "/".join(splitHeader.split("/")[1:]) + ","
              
              splitEvents[0] = newHeader[:-1]

            return splitEvents


          # Handle CSV to JSON conversion, and optionally remove null fields
          def csvToJSON(splitEvents):

            newEvents = []

            # Change CSVs with headers to JSON format
            csvSplit = csv.DictReader(splitEvents)
            for csvRow in csvSplit: 
              newEvents.append(csvRow)

            # Remove JSON fields with null or no value
            if SPLUNK_REMOVE_EMPTY_CSV_TO_JSON_FIELDS == "true":

              newEventsWithoutEmptyValues = []
              for newEvent in newEvents:
                newEventWithoutEmptyValues = {}
                
                for newEventKey in newEvent.keys():
                  if len(newEvent[newEventKey]) > 0:
                    newEventWithoutEmptyValues[newEventKey] = newEvent[newEventKey]

                newEventsWithoutEmptyValues.append(newEventWithoutEmptyValues)

              newEvents.clear()
              return newEventsWithoutEmptyValues

            return newEvents


          # Set timestamp on event
          def getTimestamp(event, delimiter):

            try:
              match SPLUNK_TIME_FORMAT:
                case "prefix-ISO8601": # For ISO8601 (%Y-%m-%dT%H-%M-%S.%fZ)
                  
                  if len(SPLUNK_TIME_PREFIX) > 0:
                    regexGroupIndex = 2
                  else: 
                    regexGroupIndex = 0

                  iso8601Timestamp = re.search("" + SPLUNK_TIME_PREFIX + r"(.{1,5})?(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(.\d{0,10})?Z)", str(event)).group(regexGroupIndex) #fix eventTime
                  return dateutil.parser.parse(iso8601Timestamp).timestamp()

                case "prefix-epoch":# For prefix epoch formats
                  
                  epochTimeString = re.search("" + SPLUNK_TIME_PREFIX + r"(.{1,5})?\d{10,13}", str(event)).group(0)
                  epochTime = re.search(r"\d{10,13}", str(epochTimeString)).group(0)
                  if len(epochTime) == 13:
                    epochTime = float(epochTime) / 1000

                  return float(epochTime)

                case "delineated-epoch": # For field-delimited epoch time
                  
                  epochTime = float(event.split(delimiter)[int(SPLUNK_TIME_DELINEATED_FIELD)])
                  return epochTime

                case "delineated-ISO8601": # For delineated ISO8601 (%Y-%m-%dT%H-%M-%S.%fZ)
                  
                  iso8601Timestamp = event.split(delimiter)[int(SPLUNK_TIME_DELINEATED_FIELD)]
                  return dateutil.parser.parse(iso8601Timestamp).timestamp()
                
                case "delineated-strftime": # For custom strftime formats
                  
                  rawTimeStamp = event.split(delimiter)[int(SPLUNK_TIME_DELINEATED_FIELD)]
                  return int(datetime.datetime.strptime(rawTimeStamp, SPLUNK_STRFTIME_FORMAT).strftime("%s"))
            
            except:
              # If not standard, set to current time
              print("Unable to extract timestamp.  Falling back to current time.")
              return time.time()

            return time.time()


          # Buffer and send events to Firehose
          def bufferAndSendEventsToFirehose(event, final, objectName, eventBatch):

            # Add current event ot recordBatch
            if len(event) > 0: # This will be 0 if it's a final call to clear the buffer
              recordBatch.append({"Data": event})

            # Attempt to send until maxRetriesToFirehose is hit 
            sendingAttempt = 1
            while sendingAttempt <= maxRetriesToFirehose:

              try:
                # If there are more than 450 records or 4500000B in the sending queue or this is the final sending and there are records to send, send the event to Splunk and clear the queue
                if len(recordBatch) >= 450 or sys.getsizeof(recordBatch) >= 4500000 or (final == True and len(recordBatch) >= 1):
                  
                  # Incrmenet eventBatch for logging
                  if sendingAttempt == 1:
                    eventBatch[0] += 1
                  
                  # Send the event batch
                  response = firehoseClient.put_record_batch(DeliveryStreamName=firehoseDeliverySreamName, Records=recordBatch)

                  # If no messages failed...
                  if response['FailedPutCount'] == 0:
                    recordBatch.clear()
                    return("Sent to Firehose")
                  # If messages failed...
                  else:
                    print("Unable to send file to Firehose. Error message: " + response['RequestResponses'][0]['ErrorMessage'])

                # If the threshold for sending wasn't reached...
                else:
                  if (final == True and len(recordBatch) == 0):
                    return("Final try, no events buffered")
                  else: 
                    return("Event buffered")

              # Print exception for debugging
              except Exception as e:
                print("Unable to send file to Firehose: " + str(e))

              # Exponential backoff with jitter
              sendingAttempt += 1
              sleepTime = 2 ** sendingAttempt
              jitter = random.uniform(0, sleepTime)
              totalSleepTime = sleepTime + jitter
              print("Retrying in " + str(round(totalSleepTime, 2)) + " seconds for attempt " + str(sendingAttempt) + " on eventBatch " + str(eventBatch[0]) + " for object " + objectName)
              time.sleep(totalSleepTime)

            # Open failure with max retries being reached
            return "Max firehose retries reached"


          # Default Lambda handler
          def handler(event, context):

            # Create delineated field break
            delimiter = createDelimiter(SPLUNK_EVENT_DELIMITER)

            # Loop through each SQS message
            for message in event['Records']:

              # Retrieve bucket name and key from SQS message
              objectInfo = retrieveObjectInfo(message)

              # Set eventBatch value, on a per-object basis
              eventBatch = [0]

              # If a string was returned instead of a dictionary, print the error and stop this loop
              if isinstance(objectInfo, str):
                print(objectInfo)
                continue

              # Validate file types
              isValidFileTypeResult = isValidFileType(objectInfo["key"])
              if not isValidFileTypeResult:
                print("Unsupported file type: s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])
                continue
              
              # Retrieve the S3 object and uncompress it
              downloadResult = downloadS3Object(objectInfo["bucket"], objectInfo["key"])
              
              # If the file was unable to be downloaded, print the error and stop this loop
              if "Unable to download" in downloadResult:
                print(downloadResult)
                continue

              # Send file info to be uncompressed
              uncompressResult = uncompressFile(downloadResult)

              # If the file was unable to be compressed, print the error and stop this loop
              if "Unable to uncompress file" in uncompressResult:
                print("Unable to uncompress file s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])
                continue

              # Try to read the file contents into memory
              try:
                with open(uncompressResult, 'r') as f:
                  events = f.read()
              except:
                print("Unable to read file contents into memory")
                continue

              # Set extension 
              extension = uncompressResult.split(".")[-1]

              # Split events
              splitEvents = eventBreak(events, extension, SPLUNK_IGNORE_FIRST_LINE)

              # Clean up first line of events
              if SPLUNK_SOURCETYPE == "aws:billing:cur":
                splitEvents = cleanFirstLine(splitEvents)

              # If a string was returned instead of a list, print the error and stop this loop
              if isinstance(splitEvents, str):
                print("File type unsupported s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])
                continue

              # Transform CSV to JSON
              if SPLUNK_CSV_TO_JSON == "true":
                splitEvents = csvToJSON(splitEvents)

              # Loop through split events
              for splitEvent in splitEvents:

                # Get timestamp
                timestamp = getTimestamp(splitEvent, delimiter)

                # Construct event to send to Splunk
                splunkEvent = '{ "time": ' +  str(timestamp) + ', "host": "' + SPLUNK_HOST + '", "source": "' + SPLUNK_SOURCE + '", "sourcetype": "' + SPLUNK_SOURCETYPE + '", "index": "' + SPLUNK_INDEX + '", "event":  ' + json.dumps(splitEvent) + ' }'

                # Buffer and send the events to Firehose
                result = bufferAndSendEventsToFirehose(str(splunkEvent), False, objectInfo["key"], eventBatch)

                # Error logging
                if result.startswith("Unable to send file to Firehose"):
                  print(result + " Firehose name: " + firehoseDeliverySreamName + ". File path: s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])

              # Send the remaining events to Firehose, effectively clearing the buffered events in recordBatch
              bufferAndSendEventsToFirehose("", True, objectInfo["key"], eventBatch)

              # Delete the file to clear up space in /tmp to make room for the next one
              os.remove(uncompressResult)

              # Logging
              print("Processed file s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])
      Description: Lambda function for processing SQS messages that contain events, then sending them to firehose to be forwarded to Splunk.
      Environment:
        Variables:
          firehoseDeliverySreamName: !Ref firehose
          SPLUNK_INDEX: !Ref splunkIndex
          SPLUNK_TIME_PREFIX: !Ref splunkTimePrefix
          SPLUNK_EVENT_DELIMITER: !Ref splunkEventDelimiter
          SPLUNK_TIME_DELINEATED_FIELD: !Ref splunkTimeDelineatedField
          SPLUNK_TIME_FORMAT: !Ref splunkTimeFormat
          SPLUNK_STRFTIME_FORMAT: !Ref splunkStrfTimeFormat
          SPLUNK_SOURCETYPE: !Ref splunkSourcetype
          SPLUNK_SOURCE: !Ref splunkSource
          SPLUNK_HOST: !Ref splunkHost
          SPLUNK_JSON_FORMAT: !Ref splunkJSONFormat
          SPLUNK_IGNORE_FIRST_LINE: !Ref splunkIgnoreFirstLine
          SPLUNK_CSV_TO_JSON: !Ref splunkCSVToJSON
          SPLUNK_REMOVE_EMPTY_CSV_TO_JSON_FIELDS: !Ref splunkRemoveEmptyCSVToJsonFields
      FunctionName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function"
      Handler: index.handler
      MemorySize: !Ref lambdaProcessorMemorySize
      Layers:
        - !Sub "arn:aws:lambda:${AWS::Region}:336392948345:layer:AWSSDKPandas-Python312-Arm64:16"
      Role: !GetAtt lambdaIAMRole.Arn
      Runtime: python3.12
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact
      Timeout: !Ref lambdaProcessorTimeout

  lambdaIAMPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action:
          - s3:GetObject
          Resource: !If [useExistingS3Bucket, !Sub "arn:aws:s3:::${existingS3BucketName}/*", !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"]
        - Effect: Allow
          Action:
          - firehose:PutRecord
          - firehose:PutRecordBatch
          Resource: !Sub "arn:aws:firehose:${AWS::Region}:${AWS::AccountId}:deliverystream/${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
        - Effect: Allow
          Action:
          - logs:CreateLogStream
          - logs:PutLogEvents
          Resource: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function*"
        - Effect: Allow
          Action:
          - sqs:ListQueues
          - sqs:GetQueueAttributes
          - sqs:ReceiveMessage
          - sqs:DeleteMessage
          - sqs:DeleteMessageBatch
          - sqs:ChangeMessageVisibility
          Resource: !Sub "arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:${AWS::AccountId}-${AWS::Region}-${logType}-sqs-queue"
      ManagedPolicyName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-iam-policy"

  lambdaIAMRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: "Allow"
          Principal:
            Service: "lambda.amazonaws.com"
          Action:
            - sts:AssumeRole
      ManagedPolicyArns:
        - !Ref lambdaIAMPolicy
      RoleName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-iam-role"
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact

  lambdaLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function"
      RetentionInDays: 7

  lambdaEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      BatchSize: !Ref lambdaProcessorBatchSize
      Enabled: true
      EventSourceArn: !Sub "arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:${AWS::AccountId}-${AWS::Region}-${logType}-sqs-queue"
      FunctionName: !GetAtt lambdaFunction.Arn
      MaximumBatchingWindowInSeconds: !Ref lambdaProcessorBatchingWindowInSeconds

  # Monitoring resoruces
  monitoringSNSTopic:
    Condition: enableAlerting
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-alerting-topic"
      TopicName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-alerting-topic"
      Subscription:
        - Endpoint: !Ref cloudWatchAlertEmail
          Protocol: email

  lambdaInvocationAlarm:
    Condition: enableAlerting
    Type: AWS::CloudWatch::Alarm
    Properties:
      ActionsEnabled: True
      AlarmActions: 
        - !Ref monitoringSNSTopic
      AlarmDescription: !Sub "Alarm if lambda function ${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function errors out.  Check CloudWatch Logs to verify the function is running correctly."
      AlarmName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-error-invocations"
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
      - Name: FunctionName
        Value: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function"
      EvaluationPeriods: 1
      MetricName: Errors
      Namespace: AWS/Lambda
      Period: 60
      Statistic: Sum
      Threshold: 1
      Unit: Count

  lambdaDurationAlarm:
    Condition: enableAlerting
    Type: AWS::CloudWatch::Alarm
    Properties:
      ActionsEnabled: True
      AlarmActions: 
        - !Ref monitoringSNSTopic
      AlarmDescription: !Sub "Alarm if lambda function ${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function runs over 5 minutes.  Consider tuning the lambdaProcessorBatchSize and lambdaProcessorBatchingWindowInSeconds parameters."
      AlarmName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-error-duration"
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
      - Name: FunctionName
        Value: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function"
      EvaluationPeriods: 1
      MetricName: Duration
      Namespace: AWS/Lambda
      Period: 60
      Statistic: Maximum
      Threshold: 300000
      Unit: Milliseconds

  firehoseDeliveryAlarm:
    Condition: enableAlerting
    Type: AWS::CloudWatch::Alarm
    Properties:
      ActionsEnabled: True
      AlarmActions: 
        - !Ref monitoringSNSTopic
      AlarmDescription: !Sub "Alarm if Firehose ${AWS::AccountId}-${AWS::Region}-${logType}-firehose cannot deliver to Splunk."
      AlarmName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-delivery-alarm"
      ComparisonOperator: LessThanThreshold
      Dimensions:
      - Name: DeliveryStreamName
        Value: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
      EvaluationPeriods: 1
      MetricName: DeliveryToSplunk.Success
      Namespace: AWS/Firehose
      Period: 60
      Statistic: Maximum
      Threshold: 1
      Unit: Count

  firehoseThrottlingAlarm:
    Condition: enableAlerting
    Type: AWS::CloudWatch::Alarm
    Properties:
      ActionsEnabled: True
      AlarmActions: 
        - !Ref monitoringSNSTopic
      AlarmDescription: !Sub "Alarm if Firehose ${AWS::AccountId}-${AWS::Region}-${logType}-firehose is throttling events because of quota limits."
      AlarmName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-throttling-alarm"
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
      - Name: DeliveryStreamName
        Value: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
      EvaluationPeriods: 1
      MetricName: ThrottledRecords
      Namespace: AWS/Firehose
      Period: 60
      Statistic: Maximum
      Threshold: 0
      Unit: Count

Outputs:
  s3BucketArn:
    Condition: createNewS3Bucket
    Value: !GetAtt s3Bucket.Arn
  s3BucketNotificationSQSQueueArn:
    Value: !GetAtt s3BucketNotificationSQSQueue.Arn
  firehoseArn:
    Value: !GetAtt firehose.Arn
  firehoseIAMRoleArn:
    Value: !GetAtt firehoseIAMRole.Arn
  firehoseBacksplashBucketArn:
    Value: !GetAtt firehoseBacksplashBucket.Arn
  firehoseLogGroupArn:
    Value: !GetAtt firehoseLogGroup.Arn
  lambdaFunctionArn:
    Value: !GetAtt lambdaFunction.Arn
  lambdaIAMRoleArn:
    Value: !GetAtt lambdaIAMRole.Arn
  lambdaLogGroupArn:
    Value: !GetAtt lambdaLogGroup.Arn
  monitoringSNSTopicArn:
    Condition: enableAlerting
    Value: !Ref monitoringSNSTopic