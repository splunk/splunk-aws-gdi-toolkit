---
AWSTemplateFormatVersion: 2010-09-09
Description: This is a CloudFormation template to create infrastructure to send logs that are put into an S3 bucket to Splunk.

Parameters:

  service:
    Type: String
    Description: Service name used in tagging AWS resources.
    Default: splunk-aws-gdi-tooklit

  stage:
    Type: String
    Description: Used to distinguish between stages of an environment (dev, test, prod, stage, etc).  Only used in AWS resource tagging.
    Default: dev

  contact:
    Description: Used to identify a contact for the resources created in this stack.  Only used in AWS resource tagging.  As an example, this could be an email address or username.
    Type: String
    Default: ""

  logType:
    Type: String
    Description: Log type, used in AWS resource names.  Must follow the normal rules for S3 bucket names (https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html), but also less than 30 characters long.
    AllowedPattern: "(?!(xn--|-s3alias$))^[a-z0-9][a-z0-9-]{1,20}[a-z0-9]$"

  existingS3BucketName:
    Type: String
    Description: Name of the existng S3 bucket you want to ingest data from.  Leave this blank to create a new S3 bucket.  If you specify the name of an existing S3 bucket, you'll need to create the S3 > SQS notification settings.
    Default: ""

  s3ObjectExpirationInDays:
    Description: How many days to keep the files in S3 before they're deleted.  This only applies to new buckets and not existing buckets.
    Default: 90
    Type: String

  roleAccessToS3BucketPut:
    Type: String
    Description: ARN of the role to allow the putting of objects in the S3 bucket.  Leave blank to auto-detect appropriate settings for CloudTrail, ELB Access, or VPC Flow logs.
    Default: ""

  sqsQueueVisibilityTimeoutInSecond:
    Type: Number
    Description: How long to let SQS messages be taken by the Lambda function before they become available to be processed again.  Must be more than lambdaProcessorTimeout.
    Default: 630
    MinValue: 0
    MaxValue: 43200

  splunkHECEndpoint:
    Type: String
    Description: Destination URL that Firehose will send data to.

  splunkHECToken:
    Type: String
    Description: HEC token Firehose will use to authenticate data being sent to Splunk.

  lambdaProcessorMemorySize:
    Type: Number
    Description: Size of memory to allocate to Lambda function.
    Default: 1024
    MinValue: 128
    MaxValue: 10240

  lambdaProcessorTimeout:
    Type: Number
    Description: How long the Lambda function can run until it times out.
    Default: 300
    MinValue: 1
    MaxValue: 900

  lambdaProcessorBatchSize:
    Type: Number
    Description: How many SQS messages (aka files from S3) to process in a single Lambda execution.
    Default: 100
    MinValue: 1
    MaxValue: 10000

  lambdaProcessorBatchingWindowInSeconds:
    Type: Number
    Description: How long to let SQS messages (aka files from S3) queue up before processing them.
    Default: 15
    MinValue: 0
    MaxValue: 300

  splunkIndex:
    Type: String
    Description: Name of the index in Splunk events will be sent to.

  splunkSourcetype:
    Type: String
    Description: Name of the sourcetype for the events that will be sent to Splunk.

  splunkSource:
    Type: String
    Description: Name of the source for the events that will be sent to Splunk.

  splunkHost:
    Type: String
    Description: Name of the host for the events that will be sent to Splunk.

  splunkTimePrefix:
    Type: String
    Description: Text right before the time stamp in the event that will be used to help identify the timestamp.  Leave empty if the event is delineated.
    Default: ""

  splunkEventDelimiter:
    Type: String
    Description: Text that is used to split the fields in each event.  Leave empty if the log file is not delineated.
    Default: ""

  splunkTimeDelineatedField:
    Type: String
    Description: Number associated with a delineated event that corresponds with the timestamp.  Leave empty if the log file is not delineated.
    Default: ""

  splunkTimeFormat:
    Type: String
    Description: How the timestamp in the event is formatted.
    Default: prefix-ISO8601
    AllowedValues:
      - prefix-ISO8601
      - prefix-epoch
      - delineated-epoch
      - delineated-ISO8601
      - delineated-strftime

  splunkStrfTimeFormat:
    Type: String
    Description: Strftime format of timestamp.  Leave empty if the timestamp does not need to be specified in strftime.
    Default: ""

  splunkJSONFormat:
    Type: String
    Description: How the JSON events are formatted.  Leave default if the events are not in JSON format.
    Default: NDJSON
    AllowedValues:
      - eventsInRecords
      - NDJSON

  splunkIgnoreFirstLine:
    Type: String
    Description: Whether the first line of the log file should be discarded.
    Default: false
    AllowedValues:
      - true
      - false

  splunkCSVToJSON:
    Type: String
    Description: Whether the to use the first line in the CSV file to convert the file from CSV to JSON.  If the file is not CSV-formatted, this can be ignored and left at the default setting.
    Default: false
    AllowedValues:
      - true
      - false

  splunkRemoveEmptyCSVToJsonFields:
    Type: String
    Description: Whether the to remove empty or null JSON fields after converting the event from CSV to JSON.  If the file is not CSV-formatted, this can be ignored and left at the default setting.
    Default: true
    AllowedValues:
      - true
      - false

  cloudWatchAlertEmail:
    Type: String
    Description: Email address for receiving alerts related to CloudTrail ingestion.  Leave empty for no alerting.
    Default: ""


Conditions:
  createNewS3Bucket: !Equals
    - !Ref existingS3BucketName
    - ""
  useExistingS3Bucket: !Not
    - !Equals
      - !Ref existingS3BucketName
      - ""
  cloudTrail: !Equals
    - !Ref logType
    - "cloudtrail"
  cloudTrailNewBucket: !And
    - !Condition cloudTrail
    - !Condition createNewS3Bucket
  vpcFlowLog: !Equals
    - !Ref logType
    - "vpcflowlog"
  vpcFlowLogNewBucket: !And
    - !Condition vpcFlowLog
    - !Condition createNewS3Bucket
  elbLog: !Equals
    - !Ref logType
    - "elblog"
  elbLogNewBucket: !And
    - !Condition elbLog
    - !Condition createNewS3Bucket
  route53Log: !Equals
    - !Ref logType
    - "route53"
  route53LogNewBucket: !And
    - !Condition route53Log
    - !Condition createNewS3Bucket
  billingLog: !Equals
    - !Ref logType
    - "billingcur"
  billingNewBucket: !And
    - !Condition billingLog
    - !Condition createNewS3Bucket
  s3ServerAccessLog: !Equals
    - !Ref logType
    - "s3serveraccesslog"
  s3ServerAccessLogNewBucket: !And
    - !Condition s3ServerAccessLog
    - !Condition createNewS3Bucket
  customLogType: !And
    - !Not
       - !Condition cloudTrail
    - !Not
       - !Condition vpcFlowLog
    - !Not
       - !Condition elbLog
    - !Not
       - !Condition route53Log
    - !Not
       - !Condition billingLog
    - !Not
       - !Condition s3ServerAccessLog
  customLogTypeNewBucket: !And
    - !Condition customLogType
    - !Condition createNewS3Bucket
  enableAlerting: !Not 
    - !Equals 
      - !Ref cloudWatchAlertEmail
      - ""


Mappings:
  RegionMapping:
    us-east-1:
      elbAccountIDPrincipal: arn:aws:iam::127311923021:root
    us-east-2:
      elbAccountIDPrincipal: arn:aws:iam::033677994240:root
    us-west-1:
      elbAccountIDPrincipal: arn:aws:iam::027434742980:root
    us-west-2:
      elbAccountIDPrincipal: arn:aws:iam::797873946194:root
    af-south-1:
      elbAccountIDPrincipal: arn:aws:iam::098369216593:root
    ca-central-1:
      elbAccountIDPrincipal: arn:aws:iam::985666609251:root
    eu-central-1:
      elbAccountIDPrincipal: arn:aws:iam::054676820928:root
    eu-west-1:
      elbAccountIDPrincipal: arn:aws:iam::156460612806:root
    eu-west-2:
      elbAccountIDPrincipal: arn:aws:iam::652711504416:root
    eu-south-1:
      elbAccountIDPrincipal: arn:aws:iam::635631232127:root
    eu-west-3:
      elbAccountIDPrincipal: arn:aws:iam::009996457667:root
    eu-north-1:
      elbAccountIDPrincipal: arn:aws:iam::897822967062:root
    ap-east-1:
      elbAccountIDPrincipal: arn:aws:iam::754344448648:root
    ap-northeast-1:
      elbAccountIDPrincipal: arn:aws:iam::582318560864:root
    ap-northeast-2:
      elbAccountIDPrincipal: arn:aws:iam::600734575887:root
    ap-northeast-3:
      elbAccountIDPrincipal: arn:aws:iam::383597477331:root
    ap-southeast-1:
      elbAccountIDPrincipal: arn:aws:iam::114774131450:root
    ap-southeast-2:
      elbAccountIDPrincipal: arn:aws:iam::783225319266:root
    ap-southeast-3:
      elbAccountIDPrincipal: arn:aws:iam::589379963580:root
    ap-south-1:
      elbAccountIDPrincipal: arn:aws:iam::718504428378:root
    me-south-1:
      elbAccountIDPrincipal: arn:aws:iam::076674570225:root
    sa-east-1:
      elbAccountIDPrincipal: arn:aws:iam::507241528517:root
    us-gov-west-1:
      elbAccountIDPrincipal: arn:aws:iam::048591011584:root
    us-gov-east-1:
      elbAccountIDPrincipal: arn:aws:iam::190560391635:root
    cn-north-1:
      elbAccountIDPrincipal: arn:aws:iam::638102146993:root
    cn-northwest-1:
      elbAccountIDPrincipal: arn:aws:iam::037604701340:root


Resources:
  # S3 resources
  s3Bucket:
    Type: AWS::S3::Bucket
    Condition: createNewS3Bucket
    Properties:
      AccessControl: Private
      BucketEncryption: 
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      BucketName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}"
      LifecycleConfiguration:
        Rules:
            - Id: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-cleanup"
              AbortIncompleteMultipartUpload:
                DaysAfterInitiation: 1
              Status: Enabled
            - Id: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-expiration"
              ExpirationInDays: !Ref s3ObjectExpirationInDays
              Status: Enabled
      NotificationConfiguration:
        QueueConfigurations:
          - Event: "s3:ObjectCreated:*"
            Queue: !GetAtt s3BucketNotificationSQSQueue.Arn
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact
      - Key: logType
        Value: !Ref logType

  s3BucketPolicyCloudTrailLogs:
    Type: AWS::S3::BucketPolicy
    Condition: cloudTrailNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            Service: cloudtrail.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: cloudtrail.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"

  s3BucketPolicyVPCFlowLogs:
    Type: AWS::S3::BucketPolicy
    Condition: vpcFlowLogNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            Service: delivery.logs.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: delivery.logs.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              s3:x-amz-acl: bucket-owner-full-control

  s3BucketPolicyELBLogs:
    Type: AWS::S3::BucketPolicy
    Condition: elbLogNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            Service: logdelivery.elb.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            AWS: !FindInMap [RegionMapping, !Ref AWS::Region, elbAccountIDPrincipal]
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              s3:x-amz-acl: bucket-owner-full-control

  s3BucketPolicyRoute53Logs:
    Type: AWS::S3::BucketPolicy
    Condition: route53LogNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            Service: delivery.logs.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: delivery.logs.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              s3:x-amz-acl: bucket-owner-full-control
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            AWS: !Sub "arn:aws:iam::${AWS::AccountId}:root"
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"

  s3BucketPolicyBillingLogs:
    Type: AWS::S3::BucketPolicy
    Condition: billingNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
            - s3:GetBucketPolicy
          Principal: 
            Service: billingreports.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
          Condition:
            StringEquals:
              aws:SourceArn: !Sub "arn:aws:cur:us-east-1:${AWS::AccountId}:definition/*"
              aws:SourceAccount: !Ref AWS::AccountId
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: billingreports.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              aws:SourceArn: !Sub "arn:aws:cur:us-east-1:${AWS::AccountId}:definition/*"
              aws:SourceAccount: !Ref AWS::AccountId

  s3BucketPolicyS3ServerAccessLogs:
    Type: AWS::S3::BucketPolicy
    Condition: s3ServerAccessLogNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Sid: "S3ServerAccessLogsPolicy"
          Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            Service: logging.s3.amazonaws.com
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"
          Condition:
            StringEquals:
              aws:SourceAccount: !Ref AWS::AccountId

  s3BucketPolicyCustomRole:
    Type: AWS::S3::BucketPolicy
    Condition: customLogTypeNewBucket
    Properties:
      Bucket: !Ref s3Bucket
      PolicyDocument:
        Statement:
        - Effect: Allow
          Action:
            - s3:GetBucketAcl
          Principal: 
            AWS: !Ref roleAccessToS3BucketPut
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"
        - Effect: Allow
          Action:
            - s3:PutObject
          Principal: 
            AWS: !Ref roleAccessToS3BucketPut
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"

  # S3 Notifications > SQS
  s3BucketNotificationSQSQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-sqs-queue"
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact
      - Key: logType
        Value: !Ref logType
      VisibilityTimeout: !Ref sqsQueueVisibilityTimeoutInSecond

  s3BucketNotificationSQSQueuePolicy: 
    Type: AWS::SQS::QueuePolicy
    Properties: 
      PolicyDocument:
        Version: 2012-10-17
        Id: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-sqs-queuePolicy"
        Statement:
        -
          Sid: Send messages to SQS
          Effect: Allow
          Principal:
            AWS: "*"
          Action:
            - "SQS:SendMessage"
          Resource: "*"
          Condition:
            ArnLike: 
              "aws:SourceARN": !If [useExistingS3Bucket, !Sub "arn:aws:s3:::${existingS3BucketName}", !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}"]
      Queues:
        - !Ref "s3BucketNotificationSQSQueue"

  # Firehose > Splunk resources
  firehose:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties: 
      DeliveryStreamName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
      DeliveryStreamType: DirectPut
      SplunkDestinationConfiguration:
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref firehoseLogGroup
          LogStreamName: "SplunkDelivery"
        HECAcknowledgmentTimeoutInSeconds: 300
        HECEndpoint: !Ref splunkHECEndpoint
        HECEndpointType: "Event"
        HECToken: !Ref splunkHECToken
        RetryOptions:
          DurationInSeconds: 3600
        S3BackupMode: "FailedEventsOnly"
        S3Configuration:
          BucketARN: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}-firehose-backsplash"
          BufferingHints:
            IntervalInSeconds: 300
            SizeInMBs: 5
          CompressionFormat: "UNCOMPRESSED"
          Prefix: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
          RoleARN: !GetAtt firehoseIAMRole.Arn

  firehoseIAMPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action:
          - logs:Describe*
          - logs:PutLogEvents
          Resource: !GetAtt firehoseLogGroup.Arn
        - Effect: Allow
          Action:
          - s3:PutObject
          Resource: !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}-firehose-backsplash/*"
      ManagedPolicyName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-iam-policy"

  firehoseIAMRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: "Allow"
          Principal:
            Service: "firehose.amazonaws.com"
          Action:
            - sts:AssumeRole
      ManagedPolicyArns:
        - !Ref firehoseIAMPolicy
      RoleName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-iam-role"
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact

  firehoseBacksplashBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-backsplash"
      AccessControl: Private
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
            - Id: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-backsplash-cleanup"
              AbortIncompleteMultipartUpload:
                DaysAfterInitiation: 1
              Status: Enabled
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact

  firehoseLogGroup: 
    Type: AWS::Logs::LogGroup
    Properties: 
      LogGroupName: !Sub "/aws/kinesisfirehose/${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
      RetentionInDays: 30

  firehoseLogStream:
    Type: AWS::Logs::LogStream
    Properties: 
      LogGroupName: !Ref firehoseLogGroup
      LogStreamName: "SplunkDelivery"


# Lambda resources
  lambdaFunction:
    Type: AWS::Lambda::Function
    DependsOn: lambdaLogGroup
    Properties:
      Architectures:
        - arm64
      Code:
        ZipFile: |
          import boto3, gzip, json, os, sys, shutil, re, dateutil.parser, time, csv, datetime, pandas, pyarrow

          # AWS-related setup
          s3Client = boto3.client('s3')
          firehoseDeliverySreamName = os.environ['firehoseDeliverySreamName']
          firehoseClient = boto3.client('firehose', region_name=os.environ['AWS_REGION'])
          recordBatch = []

          # Splunk-related setup
          SPLUNK_INDEX = os.environ['SPLUNK_INDEX']
          SPLUNK_TIME_PREFIX = os.environ['SPLUNK_TIME_PREFIX']
          SPLUNK_EVENT_DELIMITER = os.environ['SPLUNK_EVENT_DELIMITER']
          SPLUNK_TIME_DELINEATED_FIELD = os.environ['SPLUNK_TIME_DELINEATED_FIELD']
          SPLUNK_TIME_FORMAT = os.environ['SPLUNK_TIME_FORMAT']
          SPLUNK_STRFTIME_FORMAT = os.environ['SPLUNK_STRFTIME_FORMAT']
          SPLUNK_SOURCETYPE = os.environ['SPLUNK_SOURCETYPE']
          SPLUNK_SOURCE = os.environ['SPLUNK_SOURCE']
          SPLUNK_HOST = os.environ['SPLUNK_HOST']
          SPLUNK_JSON_FORMAT = os.environ['SPLUNK_JSON_FORMAT']
          SPLUNK_CSV_TO_JSON = os.environ['SPLUNK_CSV_TO_JSON']
          SPLUNK_IGNORE_FIRST_LINE = os.environ['SPLUNK_IGNORE_FIRST_LINE']
          SPLUNK_REMOVE_EMPTY_CSV_TO_JSON_FIELDS = os.environ['SPLUNK_REMOVE_EMPTY_CSV_TO_JSON_FIELDS']

          # Lambda things
          validFileTypes = ["gz", "gzip", "json", "csv", "log", "parquet", "txt", "ndjson", "jsonl"]
          unsupportedFileTypes = ["CloudTrail-Digest", "billing-report-Manifest"]
          delimiterMapping = {"space": " ", "tab": "  ", "comma": ",", "semicolon": ";"}

          # Create delimiter for delimiting events
          def createDelimiter(SPLUNK_EVENT_DELIMITER):

            if (SPLUNK_EVENT_DELIMITER in delimiterMapping.keys()):
              return delimiterMapping[SPLUNK_EVENT_DELIMITER]
            else:
              return SPLUNK_EVENT_DELIMITER


          # Parse SQS message for bucket information
          def retrieveObjectInfo(record):
            
            # Try to parse the record for file information
            try:
              record = json.loads(record['body'])
              bucket = record['Records'][0]['s3']['bucket']['name']
              key = record['Records'][0]['s3']['object']['key']

              # Construct and return the result
              result = {}
              result["bucket"] = bucket
              result["key"] = key
              return result

            # Return an error if the record doesn't have a valid file defined in it
            except:
              return("SQS message did not contain S3 file information.  Record: " + str(record))


          # Check to see if the file is a valid file type
          def validateFileType(key):

            # Check for invalid file types
            for unsupportedFileType in unsupportedFileTypes:
              if (unsupportedFileType in key):
                return("Unsupported file type.")

            # Define file extension
            extension = key.split(".")[-1]

            # Check for valid file types
            for validFileType in validFileTypes:
              if (extension in validFileType):
                return("Valid file type.")

            # Check for aws:s3:accesslogs
            if (SPLUNK_SOURCETYPE == "aws:s3:accesslogs" and len(key.split(".")) == 1):
              return("Valid file type.")

            return("Unsupported file type.")


          # Retrieve the S3 object, and return the new path
          def downloadS3Object(bucket, key):

            try:
              # Define the path for the file
              path = "/tmp/" + key.split("/")[-1]

              # Download the file from the S3 bucket
              s3Client.download_file(bucket, key, path)

              # Return the new file path
              return(path)

            except:
              return("Unable to download file s3://" + bucket + "/" + key)


          # Uncompress the file if it needs to be uncompressed, then return the path and the new file extension
          def uncompressFile(path):

            # Set file extension and new file path (if it gets uncompressed)
            extension = path.split(".")[-1]
            uncompressedFilePath = path[0:(-1*(len(extension)) - 1)]

            try:
              # If gzip file...
              if (extension == "gz" or extension == "gzip"):

                # Uncompress file
                with gzip.open(path, 'rb') as f_in:
                  with open(uncompressedFilePath, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)

                # Remove the uncompressed file
                os.remove(path)

                return(uncompressedFilePath)

              # If parquet file...
              elif (extension == "parquet"):
                df = pandas.read_parquet(path)
                json_array = df.to_json(orient='records', lines=True)
                uncompressedFilePath = uncompressedFilePath + ".json"
                with open(uncompressedFilePath, "w") as f_out:
                  f_out.write(json_array)

                # Remove the uncompressed file
                os.remove(path)

                return(uncompressedFilePath)

            except:
              return("Unable to uncompress file")

            return(path)


          # Split events into a list. Additional file extensions should be added here.
          def eventBreak(events, extension, ignoreFirstLine):

            if (extension == "csv" or extension == "log" or SPLUNK_SOURCETYPE == "aws:s3:accesslogs"):
              splitEvents = events.split("\n")

              # Remove empty last line if it exists
              if (len(splitEvents[-1]) == 0):
                splitEvents = splitEvents[:-1]

              if (ignoreFirstLine == "true"):
                splitEvents = splitEvents[1:]
              
              events = ""

              return splitEvents

            elif (extension == "json" or extension == "txt" or extension=="jsonl"):
              if (SPLUNK_JSON_FORMAT == "eventsInRecords"):
                splitEvents = json.loads(events)["Records"]
                events = ""

                return splitEvents

              elif (SPLUNK_JSON_FORMAT == "NDJSON"):
                splitEvents = events.split("\n")
                events = ""
                
                if (len(splitEvents[-1]) == 0):
                  splitEvents = splitEvents[:-1]

                return splitEvents

            else: 
              return("File type invalid")


          # Clean up first line 
          def cleanFirstLine(splitEvents):

            # If the sourcetype is aws:billing:cur, remove everything before the "/" in the CSV header
            if (SPLUNK_SOURCETYPE == "aws:billing:cur"):
              
              header = splitEvents[0]
              
              newHeader = ""
              
              for splitHeader in header.split(","):
                newHeader += "/".join(splitHeader.split("/")[1:]) + ","
              
              splitEvents[0] = newHeader[:-1]

            return(splitEvents)


          # Handle CSV to JSON conversion, and optionally remove null fields
          def csvToJSON(splitEvents):

            newEvents = []

            # Change CSVs with headers to JSON format
            csvSplit = csv.DictReader(splitEvents)
            for csvRow in csvSplit: 
              newEvents.append(csvRow)

            # Remove JSON fields with null or no value
            if (SPLUNK_REMOVE_EMPTY_CSV_TO_JSON_FIELDS == "true"):

              newEventsWithoutEmptyValues = []
              for newEvent in newEvents:
                newEventWithoutEmptyValues = {}
                for newEventKey in newEvent.keys():
                  if (len(newEvent[newEventKey]) > 0):
                    newEventWithoutEmptyValues[newEventKey] = newEvent[newEventKey]
                newEventsWithoutEmptyValues.append(newEventWithoutEmptyValues)

              newEvents.clear()
              return newEventsWithoutEmptyValues

            return newEvents


          # Set timestamp on event
          def getTimestamp(event, delimiter):

            try:
              # For ISO8601 (%Y-%m-%dT%H-%M-%S.%fZ)
              if (SPLUNK_TIME_FORMAT == "prefix-ISO8601"):

                if (len(SPLUNK_TIME_PREFIX) > 0):
                  regexGroupIndex = 2
                else: 
                  regexGroupIndex = 0

                iso8601Timestamp = re.search("" + SPLUNK_TIME_PREFIX + "(.{1,5})?(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(.\d{0,10})?Z)", str(event)).group(regexGroupIndex) #fix eventTime
                return(dateutil.parser.parse(iso8601Timestamp).timestamp())
              # For prefix epoch formats
              elif (SPLUNK_TIME_FORMAT == "prefix-epoch"):
                epochTimeString = re.search("" + SPLUNK_TIME_PREFIX + "(.{1,5})?\d{10,13}", str(event)).group(0)
                epochTime = re.search("\d{10,13}", str(epochTimeString)).group(0)
                if (len(epochTime) == 13):
                  epochTime = float(epochTime) / 1000
                return(float(epochTime))
              # For field-delimited epoch time
              elif (SPLUNK_TIME_FORMAT == "delineated-epoch"):
                epochTime = float(event.split(delimiter)[int(SPLUNK_TIME_DELINEATED_FIELD)])
                return(epochTime)
              # For delineated ISO8601 (%Y-%m-%dT%H-%M-%S.%fZ)
              elif (SPLUNK_TIME_FORMAT == "delineated-ISO8601"):
                iso8601Timestamp = event.split(delimiter)[int(SPLUNK_TIME_DELINEATED_FIELD)]
                return(dateutil.parser.parse(iso8601Timestamp).timestamp())
              # For custom strftime formats
              elif(SPLUNK_TIME_FORMAT == "delineated-strftime"):
                rawTimeStamp = event.split(delimiter)[int(SPLUNK_TIME_DELINEATED_FIELD)]
                return(int(datetime.datetime.strptime(rawTimeStamp, SPLUNK_STRFTIME_FORMAT).strftime("%s")))
            except:
              # If not standard, set to current time
              print("Unable to extract timestamp.  Falling back to current time.")
              return(time.time())

            return(time.time())


          # Buffer and send events to Firehose
          def sendEventsToFirehose(event, final):

            # Add current event ot recordBatch
            if (len(event) > 0): # This will be 0 if it's a final call to clear the buffer
              recordBatch.append({"Data": event})

            try:

              # If there are more than 200 records or 2MB in the sending queue, send the event to Splunk and clear the queue
              if (len(recordBatch) > 200 or (sys.getsizeof(recordBatch) > 2000000 )):
                response = firehoseClient.put_record_batch(DeliveryStreamName=firehoseDeliverySreamName, Records=recordBatch)
                recordBatch.clear()

                if (response['FailedPutCount'] > 0):
                  return("Unable to send file to Firehose.  First error message: " + response['RequestResponses'][0]['ErrorMessage'])

              elif (final == True):
                response = firehoseClient.put_record_batch(DeliveryStreamName=firehoseDeliverySreamName, Records=recordBatch)
                recordBatch.clear()

                if (response['FailedPutCount'] > 0):
                  return("Unable to send file to Firehose.  First error message: " + response['RequestResponses'][0]['ErrorMessage'])

            except:
              return("Unable to send file to Firehose")

            return("Sent to Firehose")


          # Default Lambda handler
          def handler(event, context):

            # Create delineated field break
            delimiter = createDelimiter(SPLUNK_EVENT_DELIMITER)

            # Loop through each SQS message
            for message in event['Records']:

              # Retrieve bucket name and key from SQS message
              objectInfo = retrieveObjectInfo(message)

              # If a string was returned instead of a dictionary, print the error and stop this loop
              if (isinstance(objectInfo, str)):
                print(objectInfo)
                continue

              # Validate file types
              validateFileTypeResult = validateFileType(objectInfo["key"])
              if ("Unsupported file type." in validateFileTypeResult):
                print("Unsupported file type: s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])
                continue
              
              # Retrieve the S3 object and uncompress it
              downloadResult = downloadS3Object(objectInfo["bucket"], objectInfo["key"])
              
              # If the file was unable to be downloaded, print the error and stop this loop
              if ("Unable to download" in downloadResult):
                print(downloadResult)
                continue

              # Send file info to be uncompressed
              uncompressResult = uncompressFile(downloadResult)

              # If the file was unable to be compressed, print the error and stop this loop
              if ("Unable to uncompress file" in uncompressResult):
                print("Unable to uncompress file s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])
                continue

              # Try to read the file contents into memory
              try:
                with open(uncompressResult, 'r') as f:
                  events = f.read()
              except:
                print("Unable to read file contents into memory")
                continue

              # Set extension 
              extension = uncompressResult.split(".")[-1]

              # Split events
              splitEvents = eventBreak(events, extension, SPLUNK_IGNORE_FIRST_LINE)

              # Clean up first line of events
              if (SPLUNK_SOURCETYPE == "aws:billing:cur"):
                splitEvents = cleanFirstLine(splitEvents)

              # If a string was returned instead of a list, print the error and stop this loop
              if (isinstance(splitEvents, str)):
                print("File type unsupported s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])
                continue

              # Transform CSV to JSON
              if (SPLUNK_CSV_TO_JSON == "true"):
                splitEvents = csvToJSON(splitEvents)

              # Loop through split events
              for splitEvent in splitEvents:

                # Get timestamp
                timestamp = getTimestamp(splitEvent, delimiter)

                # Construct event to send to Splunk
                splunkEvent = '{ "time": ' +  str(timestamp) + ', "host": "' + SPLUNK_HOST + '", "source": "' + SPLUNK_SOURCE + '", "sourcetype": "' + SPLUNK_SOURCETYPE + '", "index": "' + SPLUNK_INDEX + '", "event":  ' + json.dumps(splitEvent) + ' }'

                # Buffer and send the events to Firehose
                result = sendEventsToFirehose(str(splunkEvent), False)

                # Error logging
                if (result.startswith("Unable to send file to Firehose")):
                  print(result + " Firehose name: " + firehoseDeliverySreamName + ". File path: s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])

              # Send the remaining events to Firehose, effectively clearing the buffered events in recordBatch
              sendEventsToFirehose("", True)

              # Delete the file to clear up space in /tmp to make room for the next one
              os.remove(uncompressResult)

              # Logging
              print("Processed file s3://" + objectInfo["bucket"] + "/" + objectInfo["key"])
      Description: Lambda function for processing SQS messages that contain events, then sending them to firehose to be forwarded to Splunk.
      Environment:
        Variables:
          firehoseDeliverySreamName: !Ref firehose
          SPLUNK_INDEX: !Ref splunkIndex
          SPLUNK_TIME_PREFIX: !Ref splunkTimePrefix
          SPLUNK_EVENT_DELIMITER: !Ref splunkEventDelimiter
          SPLUNK_TIME_DELINEATED_FIELD: !Ref splunkTimeDelineatedField
          SPLUNK_TIME_FORMAT: !Ref splunkTimeFormat
          SPLUNK_STRFTIME_FORMAT: !Ref splunkStrfTimeFormat
          SPLUNK_SOURCETYPE: !Ref splunkSourcetype
          SPLUNK_SOURCE: !Ref splunkSource
          SPLUNK_HOST: !Ref splunkHost
          SPLUNK_JSON_FORMAT: !Ref splunkJSONFormat
          SPLUNK_IGNORE_FIRST_LINE: !Ref splunkIgnoreFirstLine
          SPLUNK_CSV_TO_JSON: !Ref splunkCSVToJSON
          SPLUNK_REMOVE_EMPTY_CSV_TO_JSON_FIELDS: !Ref splunkRemoveEmptyCSVToJsonFields
      FunctionName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function"
      Handler: index.handler
      MemorySize: !Ref lambdaProcessorMemorySize
      Layers:
        - !Sub "arn:aws:lambda:${AWS::Region}:336392948345:layer:AWSSDKPandas-Python312-Arm64:3"
      Role: !GetAtt lambdaIAMRole.Arn
      Runtime: python3.12
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact
      Timeout: !Ref lambdaProcessorTimeout

  lambdaIAMPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action:
          - s3:GetObject
          Resource: !If [useExistingS3Bucket, !Sub "arn:aws:s3:::${existingS3BucketName}/*", !Sub "arn:aws:s3:::${AWS::AccountId}-${AWS::Region}-${logType}/*"]
        - Effect: Allow
          Action:
          - firehose:PutRecord
          - firehose:PutRecordBatch
          Resource: !Sub "arn:aws:firehose:${AWS::Region}:${AWS::AccountId}:deliverystream/${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
        - Effect: Allow
          Action:
          - logs:CreateLogStream
          - logs:PutLogEvents
          Resource: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function*"
        - Effect: Allow
          Action:
          - sqs:ListQueues
          - sqs:GetQueueAttributes
          - sqs:ReceiveMessage
          - sqs:DeleteMessage
          - sqs:DeleteMessageBatch
          - sqs:ChangeMessageVisibility
          Resource: !Sub "arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:${AWS::AccountId}-${AWS::Region}-${logType}-sqs-queue"
      ManagedPolicyName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-iam-policy"

  lambdaIAMRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: "Allow"
          Principal:
            Service: "lambda.amazonaws.com"
          Action:
            - sts:AssumeRole
      ManagedPolicyArns:
        - !Ref lambdaIAMPolicy
      RoleName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-iam-role"
      Tags:
      - Key: service
        Value: !Ref service
      - Key: stage
        Value: !Ref stage
      - Key: contact
        Value: !Ref contact

  lambdaLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function"
      RetentionInDays: 7

  lambdaEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      BatchSize: !Ref lambdaProcessorBatchSize
      Enabled: true
      EventSourceArn: !Sub "arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:${AWS::AccountId}-${AWS::Region}-${logType}-sqs-queue"
      FunctionName: !GetAtt lambdaFunction.Arn
      MaximumBatchingWindowInSeconds: !Ref lambdaProcessorBatchingWindowInSeconds

  # Monitoring resoruces
  monitoringSNSTopic:
    Condition: enableAlerting
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-alerting-topic"
      TopicName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-alerting-topic"
      Subscription:
        - Endpoint: !Ref cloudWatchAlertEmail
          Protocol: email

  lambdaInvocationAlarm:
    Condition: enableAlerting
    Type: AWS::CloudWatch::Alarm
    Properties:
      ActionsEnabled: True
      AlarmActions: 
        - !Ref monitoringSNSTopic
      AlarmDescription: !Sub "Alarm if lambda function ${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function errors out.  Check CloudWatch Logs to verify the function is running correctly."
      AlarmName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-error-invocations"
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
      - Name: FunctionName
        Value: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function"
      EvaluationPeriods: 1
      MetricName: Errors
      Namespace: AWS/Lambda
      Period: 60
      Statistic: Sum
      Threshold: 1
      Unit: Count

  lambdaDurationAlarm:
    Condition: enableAlerting
    Type: AWS::CloudWatch::Alarm
    Properties:
      ActionsEnabled: True
      AlarmActions: 
        - !Ref monitoringSNSTopic
      AlarmDescription: !Sub "Alarm if lambda function ${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function runs over 5 minutes.  Consider tuning the lambdaProcessorBatchSize and lambdaProcessorBatchingWindowInSeconds parameters."
      AlarmName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-error-duration"
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
      - Name: FunctionName
        Value: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-lambda-function"
      EvaluationPeriods: 1
      MetricName: Duration
      Namespace: AWS/Lambda
      Period: 60
      Statistic: Maximum
      Threshold: 300000
      Unit: Milliseconds

  firehoseDeliveryAlarm:
    Condition: enableAlerting
    Type: AWS::CloudWatch::Alarm
    Properties:
      ActionsEnabled: True
      AlarmActions: 
        - !Ref monitoringSNSTopic
      AlarmDescription: !Sub "Alarm if Firehose ${AWS::AccountId}-${AWS::Region}-${logType}-firehose cannot deliver to Splunk."
      AlarmName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-delivery-alarm"
      ComparisonOperator: LessThanThreshold
      Dimensions:
      - Name: DeliveryStreamName
        Value: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
      EvaluationPeriods: 1
      MetricName: DeliveryToSplunk.Success
      Namespace: AWS/Firehose
      Period: 60
      Statistic: Maximum
      Threshold: 1
      Unit: Count

  firehoseThrottlingAlarm:
    Condition: enableAlerting
    Type: AWS::CloudWatch::Alarm
    Properties:
      ActionsEnabled: True
      AlarmActions: 
        - !Ref monitoringSNSTopic
      AlarmDescription: !Sub "Alarm if Firehose ${AWS::AccountId}-${AWS::Region}-${logType}-firehose is throttling events because of quota limits."
      AlarmName: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose-throttling-alarm"
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
      - Name: DeliveryStreamName
        Value: !Sub "${AWS::AccountId}-${AWS::Region}-${logType}-firehose"
      EvaluationPeriods: 1
      MetricName: ThrottledRecords
      Namespace: AWS/Firehose
      Period: 60
      Statistic: Maximum
      Threshold: 0
      Unit: Count

Outputs:
  s3BucketArn:
    Condition: createNewS3Bucket
    Value: !GetAtt s3Bucket.Arn
  s3BucketNotificationSQSQueueArn:
    Value: !GetAtt s3BucketNotificationSQSQueue.Arn
  firehoseArn:
    Value: !GetAtt firehose.Arn
  firehoseIAMRoleArn:
    Value: !GetAtt firehoseIAMRole.Arn
  firehoseBacksplashBucketArn:
    Value: !GetAtt firehoseBacksplashBucket.Arn
  firehoseLogGroupArn:
    Value: !GetAtt firehoseLogGroup.Arn
  lambdaFunctionArn:
    Value: !GetAtt lambdaFunction.Arn
  lambdaIAMRoleArn:
    Value: !GetAtt lambdaIAMRole.Arn
  lambdaLogGroupArn:
    Value: !GetAtt lambdaLogGroup.Arn
  monitoringSNSTopicArn:
    Condition: enableAlerting
    Value: !Ref monitoringSNSTopic